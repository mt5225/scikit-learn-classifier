{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TuringBench](https://alan-turing-institute.github.io/data-science-benchmarking/) Workflow: Example #1\n",
    "====\n",
    "\n",
    "**Algorithm:** Support Vector Classification of MNIST digit images with [scikit-learn](https://scikit-learn.org)\n",
    "\n",
    "**Benchmarks:** Compare training time, prediction time and performance of the classifier on the local machine vs a Docker container on the local machine and compare different versions of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting started with TuringBench\n",
    "----\n",
    "\n",
    "This benchmarking notebook follows the TuringBench workflow outlined at https://alan-turing-institute.github.io/data-science-benchmarking/\n",
    "\n",
    "1. [Install Docker](https://docs.docker.com/v17.09/engine/installation/) on each computing platform for which you wish to carry out benchmarking\n",
    "2. Familiarise yourself with the basics of [how Dockerfile instructions work](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)\n",
    "3. Create an account on [Docker Hub](https://hub.docker.com/)\n",
    "4. If you wish to use [automated builds](https://docs.docker.com/docker-hub/builds/), ensure your software is maintained with a GitHub repository.\n",
    "\n",
    "Code for this example is maintained at https://github.com/edwardchalstrey1/scikit-learn-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC code to use as a benchmarking example\n",
    "---\n",
    "\n",
    "For the purpose of this example, consider the model generated by scikit-learn with ```svm.SVC``` in the code below to be the algorithm that we wish to benchmark.\n",
    "\n",
    "The code below trains the SVC model with half the data from the scikit-learn's MNIST image dataset and predicts with the other half. The training time, prediction time and performance are recorded as the benchmarks that we wish to collect.\n",
    "\n",
    "Using the magic writefile command, I save the contents of the cell as a file, so we can use the exact same code in the Docker version later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifier.py\n",
    "from sklearn import datasets, svm, metrics\n",
    "import time\n",
    "import statistics as st\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    return svm.SVC(gamma=0.01)\n",
    "\n",
    "def benchmark_model(model, repeats=10):\n",
    "    \n",
    "    digits = datasets.load_digits()\n",
    "\n",
    "    n_samples = len(digits.images)\n",
    "    data = digits.images.reshape((n_samples, -1)) # MNIST images\n",
    "\n",
    "    expected = digits.target[n_samples // 2:]\n",
    "\n",
    "    output = {\"Training time (s)\": [], \"Prediction time (s)\": [],\n",
    "    \"Performance (micro avg f1 score)\": []}\n",
    "    \n",
    "    tt, pt, p = [], [], []\n",
    "    \n",
    "    for i in range(0, repeats):\n",
    "        \n",
    "        # Train the classifier model\n",
    "        start = time.time()\n",
    "        model.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "        end = time.time()\n",
    "        tt.append(end - start)\n",
    "        \n",
    "        # Use the model for prediction\n",
    "        start = time.time()\n",
    "        predicted = model.predict(data[n_samples // 2:])\n",
    "        end = time.time()\n",
    "        pt.append(end - start)\n",
    "\n",
    "        p.append(metrics.classification_report(expected, predicted, output_dict=True)['micro avg']['f1-score'])\n",
    "        \n",
    "    # Get median benchmarks for chosen number of repeats\n",
    "    \n",
    "    benchmarks = {\n",
    "        \"Training time (s)\": st.median(tt),\n",
    "        \"Prediction time (s)\": st.median(pt),\n",
    "        \"Performance (micro avg f1 score)\": st.median(p)\n",
    "    }\n",
    "    \n",
    "    return benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the classifier perform on my laptop? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Training time (s)': 0.1223764419555664, 'Prediction time (s)': 0.0564265251159668, 'Performance (micro avg f1 score)': 0.6974416017797553}\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "model = classifier.create_model()\n",
    "local_results = classifier.benchmark_model(model, repeats=10)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark script\n",
    "----\n",
    "\n",
    "Let's now write a script that will print the benchmarks. This script will be used as the command in our Docker container as we'll see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting benchmarks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmarks.py \n",
    "import classifier\n",
    "model = classifier.create_model()\n",
    "print(classifier.benchmark_model(model, repeats=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Docker image for our code and push to Docker Hub\n",
    "---\n",
    "\n",
    "1) Create a docker image that installs/imports your software and runs the benchmark script you have created.\n",
    "\n",
    "2) Build a docker container and tag a version\n",
    "\n",
    "3) Push to Docker Hub. This allows you to then pull the container to any machine you wish to benchmark on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dockerfile\n",
    "\n",
    "Where possible, use a base image that has some of the requirements for your software already installed. In this example, we use the Python 3 base image and install the relevant packages with pip. Installing this simple classifier code only requires copying the ```classifier``` module we saved earlier into the container. We run the benchmarking script as the ```CMD```, so that the benchmarks will be printed when the container is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3\n",
    "\n",
    "RUN apt-get update\n",
    "RUN pip3 install numpy\n",
    "RUN pip3 install scipy\n",
    "RUN pip3 install scikit-learn\n",
    "\n",
    "COPY classifier.py /classifier.py\n",
    "COPY benchmarks.py /benchmarks.py\n",
    "\n",
    "CMD python3 benchmarks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build\n",
    "\n",
    "I've tagged this container as 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  141.3kB\r",
      "\r\n",
      "Step 1/8 : FROM python:3\n",
      " ---> ac069ebfe1e1\n",
      "Step 2/8 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 5a84d23aa7b5\n",
      "Step 3/8 : RUN pip3 install numpy\n",
      " ---> Using cache\n",
      " ---> 4383ac463a3b\n",
      "Step 4/8 : RUN pip3 install scipy\n",
      " ---> Using cache\n",
      " ---> 6fa2c9da864b\n",
      "Step 5/8 : RUN pip3 install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 0b888dbaed11\n",
      "Step 6/8 : COPY classifier.py /classifier.py\n",
      " ---> Using cache\n",
      " ---> 189c71e77002\n",
      "Step 7/8 : COPY benchmarks.py /benchmarks.py\n",
      " ---> a29ee6090d78\n",
      "Step 8/8 : CMD python3 benchmarks.py\n",
      " ---> Running in 9b30e659cd06\n",
      "Removing intermediate container 9b30e659cd06\n",
      " ---> 3fa98e268fac\n",
      "Successfully built 3fa98e268fac\n",
      "Successfully tagged edwardchalstrey/classifier:1.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -t edwardchalstrey/classifier:1.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/edwardchalstrey/classifier]\n",
      "dccb7ed6962a: Preparing\n",
      "578571cb3a28: Preparing\n",
      "f12d5b3c5c82: Preparing\n",
      "337d3babfd9c: Preparing\n",
      "493622b04a5f: Preparing\n",
      "65ef2276d16f: Preparing\n",
      "4b381ae03f9a: Preparing\n",
      "08a5b66845ac: Preparing\n",
      "88a85bcf8170: Preparing\n",
      "65860ac81ef4: Preparing\n",
      "a22a5ac18042: Preparing\n",
      "6257fa9f9597: Preparing\n",
      "578414b395b9: Preparing\n",
      "abc3250a6c7f: Preparing\n",
      "13d5529fd232: Preparing\n",
      "65ef2276d16f: Waiting\n",
      "4b381ae03f9a: Waiting\n",
      "08a5b66845ac: Waiting\n",
      "88a85bcf8170: Waiting\n",
      "65860ac81ef4: Waiting\n",
      "6257fa9f9597: Waiting\n",
      "578414b395b9: Waiting\n",
      "abc3250a6c7f: Waiting\n",
      "13d5529fd232: Waiting\n",
      "a22a5ac18042: Waiting\n",
      "493622b04a5f: Layer already exists\n",
      "337d3babfd9c: Layer already exists\n",
      "578571cb3a28: Layer already exists\n",
      "f12d5b3c5c82: Layer already exists\n",
      "65ef2276d16f: Layer already exists\n",
      "08a5b66845ac: Layer already exists\n",
      "4b381ae03f9a: Layer already exists\n",
      "88a85bcf8170: Layer already exists\n",
      "65860ac81ef4: Layer already exists\n",
      "6257fa9f9597: Layer already exists\n",
      "a22a5ac18042: Layer already exists\n",
      "578414b395b9: Layer already exists\n",
      "13d5529fd232: Layer already exists\n",
      "abc3250a6c7f: Layer already exists\n",
      "dccb7ed6962a: Pushed\n",
      "1.0: digest: sha256:775a7fe44fc3cafe4882f6a8b7e8a4e9d77816523bdf95ccfb856a6a4fbe3143 size: 3480\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push edwardchalstrey/classifier:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the docker container and collect benchmark stats\n",
    "-----\n",
    "\n",
    "The docker container is now available to be pulled and run on any computing platform with Docker installed. Here I show how we can run the container locally and get the printed results with IPython/bash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out docker_results\n",
    "docker run edwardchalstrey/classifier:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Training time (s)': 0.13670337200164795, 'Prediction time (s)': 0.06618499755859375, 'Performance (micro avg f1 score)': 0.6974416017797553}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "docker_results = ast.literal_eval(docker_results)\n",
    "print(docker_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do they compare?\n",
    "---\n",
    "\n",
    "In this example, the benchmark stats I have collected are the preformance stats measured by sci-kit learn, as well as the time taken to fit the classification model and the time it takes to predict the catagories of the test data. Let's see how these differ between my local version and the container version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Version      </td><td>Training time (s)  </td><td>Prediction time (s)</td><td>Performance (micro avg f1 score)</td></tr>\n",
       "<tr><td>Local 1.0    </td><td>0.1223764419555664 </td><td>0.0564265251159668 </td><td>0.6974416017797553              </td></tr>\n",
       "<tr><td>Container 1.0</td><td>0.13670337200164795</td><td>0.06618499755859375</td><td>0.6974416017797553              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "headers = [\"Version\"]\n",
    "c_results = [\"Local 1.0\"]\n",
    "d_results = [\"Container 1.0\"]\n",
    "for k, v in local_results.items():\n",
    "    headers.append(k)\n",
    "    c_results.append(v)\n",
    "for k, v in docker_results.items():\n",
    "    d_results.append(v)\n",
    "display(HTML(tabulate.tabulate([headers, c_results, d_results], tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking a new version of the classifier\n",
    "====\n",
    "\n",
    "Consider that you've reached a stage of your project where you now wish to benchmark a new version of your software. For the purpose of this example, consider the inspired modification to our scikit-learn classifier to be reducing the size of the ```gamma``` paramater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifier.py\n",
    "from sklearn import datasets, svm, metrics\n",
    "import time\n",
    "import statistics as st\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    return svm.SVC(gamma=0.001) # UPDATE\n",
    "\n",
    "def benchmark_model(model, repeats=10):\n",
    "    \n",
    "    digits = datasets.load_digits()\n",
    "\n",
    "    n_samples = len(digits.images)\n",
    "    data = digits.images.reshape((n_samples, -1)) # MNIST images\n",
    "\n",
    "    expected = digits.target[n_samples // 2:]\n",
    "\n",
    "    output = {\"Training time (s)\": [], \"Prediction time (s)\": [],\n",
    "    \"Performance (micro avg f1 score)\": []}\n",
    "    \n",
    "    tt, pt, p = [], [], []\n",
    "    \n",
    "    for i in range(0, repeats):\n",
    "        \n",
    "        # Train the classifier model\n",
    "        start = time.time()\n",
    "        model.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "        end = time.time()\n",
    "        tt.append(end - start)\n",
    "        \n",
    "        # Use the model for prediction\n",
    "        start = time.time()\n",
    "        predicted = model.predict(data[n_samples // 2:])\n",
    "        end = time.time()\n",
    "        pt.append(end - start)\n",
    "\n",
    "        p.append(metrics.classification_report(expected, predicted, output_dict=True)['micro avg']['f1-score'])\n",
    "        \n",
    "    # Get median benchmarks for chosen number of repeats\n",
    "    \n",
    "    benchmarks = {\n",
    "        \"Training time (s)\": st.median(tt),\n",
    "        \"Prediction time (s)\": st.median(pt),\n",
    "        \"Performance (micro avg f1 score)\": st.median(p)\n",
    "    }\n",
    "    \n",
    "    return benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New build\n",
    "\n",
    "We can then build a new container based on our Docker image and benchmark script that tests the new classifier model, which we'll tag as version 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  134.7kB\r",
      "\r\n",
      "Step 1/8 : FROM python:3\n",
      " ---> ac069ebfe1e1\n",
      "Step 2/8 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 5a84d23aa7b5\n",
      "Step 3/8 : RUN pip3 install numpy\n",
      " ---> Using cache\n",
      " ---> 4383ac463a3b\n",
      "Step 4/8 : RUN pip3 install scipy\n",
      " ---> Using cache\n",
      " ---> 6fa2c9da864b\n",
      "Step 5/8 : RUN pip3 install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 0b888dbaed11\n",
      "Step 6/8 : COPY classifier.py /classifier.py\n",
      " ---> 7fe07801e7e1\n",
      "Step 7/8 : COPY benchmarks.py /benchmarks.py\n",
      " ---> 76ef128e6e31\n",
      "Step 8/8 : CMD python3 benchmarks.py\n",
      " ---> Running in 0e629db6c3bf\n",
      "Removing intermediate container 0e629db6c3bf\n",
      " ---> e0900d204d16\n",
      "Successfully built e0900d204d16\n",
      "Successfully tagged edwardchalstrey/classifier:1.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -t edwardchalstrey/classifier:1.1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/edwardchalstrey/classifier]\n",
      "18661e28fe84: Preparing\n",
      "17e84f583592: Preparing\n",
      "f12d5b3c5c82: Preparing\n",
      "337d3babfd9c: Preparing\n",
      "493622b04a5f: Preparing\n",
      "65ef2276d16f: Preparing\n",
      "4b381ae03f9a: Preparing\n",
      "08a5b66845ac: Preparing\n",
      "88a85bcf8170: Preparing\n",
      "4b381ae03f9a: Waiting\n",
      "65860ac81ef4: Preparing\n",
      "a22a5ac18042: Preparing\n",
      "6257fa9f9597: Preparing\n",
      "578414b395b9: Preparing\n",
      "abc3250a6c7f: Preparing\n",
      "13d5529fd232: Preparing\n",
      "08a5b66845ac: Waiting\n",
      "88a85bcf8170: Waiting\n",
      "65860ac81ef4: Waiting\n",
      "a22a5ac18042: Waiting\n",
      "6257fa9f9597: Waiting\n",
      "578414b395b9: Waiting\n",
      "abc3250a6c7f: Waiting\n",
      "13d5529fd232: Waiting\n",
      "65ef2276d16f: Waiting\n",
      "493622b04a5f: Layer already exists\n",
      "f12d5b3c5c82: Layer already exists\n",
      "337d3babfd9c: Layer already exists\n",
      "18661e28fe84: Layer already exists\n",
      "17e84f583592: Layer already exists\n",
      "65ef2276d16f: Layer already exists\n",
      "08a5b66845ac: Layer already exists\n",
      "4b381ae03f9a: Layer already exists\n",
      "65860ac81ef4: Layer already exists\n",
      "88a85bcf8170: Layer already exists\n",
      "a22a5ac18042: Layer already exists\n",
      "6257fa9f9597: Layer already exists\n",
      "578414b395b9: Layer already exists\n",
      "13d5529fd232: Layer already exists\n",
      "abc3250a6c7f: Layer already exists\n",
      "1.1: digest: sha256:e060d94b67025d116b595492bc2fbfc7cd3164f7b4ccdc97c22993e64d58de9d size: 3480\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push edwardchalstrey/classifier:1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets add the benchmarks for the new classifier model into our table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out docker_results_1\n",
    "docker run edwardchalstrey/classifier:1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_results_1 = ast.literal_eval(docker_results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Version      </td><td>Training time (s)  </td><td>Prediction time (s)</td><td>Performance (micro avg f1 score)</td></tr>\n",
       "<tr><td>Local 1.0    </td><td>0.1223764419555664 </td><td>0.0564265251159668 </td><td>0.6974416017797553              </td></tr>\n",
       "<tr><td>Container 1.0</td><td>0.13670337200164795</td><td>0.06618499755859375</td><td>0.6974416017797553              </td></tr>\n",
       "<tr><td>Container 1.1</td><td>0.04636788368225098</td><td>0.03962278366088867</td><td>0.9688542825361512              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d1_results = [\"Container 1.1\"]\n",
    "for k, v in docker_results_1.items():\n",
    "    d1_results.append(v)\n",
    "display(HTML(tabulate.tabulate([headers, c_results, d_results, d1_results], tablefmt='html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
